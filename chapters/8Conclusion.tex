\chapter{Conclusion and Future Work}
\label{cha:chapter7}
This thesis investigated methods for performing fair, accurate and reproducible comparisons of forecasting methods and applied these to compare 13 forecasting methods on four public datasets. In Chapter \ref{cha:approach} the terms, fair, accurate and reproducible were defined and criterias for fullfilling them were established. These criterias were used in Chapter \ref{cha:designing} to design a system for enabling fair, accurate and reproducible comparisons. Additionally, three studies were performed in Sections \ref{sec:dataset_analysis}, \ref{sec:hypothesis_tests} and \ref{sec:ranking_distributions} to identify appropriate datasets to use, suitable hypothesis tests for reproducible results and how distributions of error metrics could be accurately compared. The datset analysis identified the M4 Daily, Electricity, Solar Energy and M5 datasets as suitable datasets as they have diverse characteristics in terms of trend and seasonality while the Kolmogorov-Smirnov test was identified as the most suitable statistical test to use for statistical reproducibility. In Section \ref{sec:ranking_distributions} the RMSE4D metric was defined and evaluated against other aggregation methods on simulated data to determine how distributions of error metrics could be accurately compared. Chapter \ref{cha:crayon} introduced Crayon, an open source benchmark for fair, accurate and reproducible comparisons of time series forecasting methods. In Section \ref{cha:crayon_case_study} the benchmarking and verification methods in Crayon was presented showing how the technical and statistical reproduciblity of Crayon benchmarks could be used to protect forecasting methods and frameworks from accuracy regressions. Not only was the intended accuracy regression successfully identified, another possible accuracy regression was found to have been introduced during the time of the first accuraccy regression. In Chapter \ref{cha:chapter6} an empirical comparison of forecasting methods took place where 13 state of the art forecasting methods were tuned and benchmarked using Crayon on the four datasets identified in \ref{sec:dataset_analysis}. The findings of this Comparison showed that DeepAR, N-BEATS and the Transformer estimators were the best performing models when ranking them based on the MASE, MAPE, MSIS, RMSE and Absolute Error metrics. However, for the M4 Daily dataset the Naive and Statistical models such as Theta outperformed all NN and hybrid approaches. This demonstrated that the "No Free Lunch" theorem holds as no single method was superior at all times.

Future improvements of the Crayon benchmark would be to add additional datasets such as those offered by Libra to enable fair comparisons to local models. Furthermore, exogenous variables such as day of the week or public holidays should be added to the datasets used when benchmarking. Further, while the RMSE4D metric introduced in this thesis is suitable for comparing distributions of error metrics it would be interesting to extend it such that it easily could be compared with single error metrics and not only other RMSE4D aggregations of a metric. Additionally extending Crayon to enable fair comparisons of multivariate forecasters would be interesting future work as these introduce additional complexity to the benchmarking system. While improvements can be made, it is important to note that the research questions of this thesis were successfully answered as methods for fair, accurate and reproducible comparisons were identifed and used for a large scale empirical comparison of forecasting methods.
