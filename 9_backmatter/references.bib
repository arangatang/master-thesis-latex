@online{gluonts_deepar_github_issue,
    author = "github user: jsirol",
    title = "DeepAR + NegativeBinomialOutput performance degradation after upgrading from v0.4.2 to 0.5.0",
    url  = "https://github.com/awslabs/gluon-ts/issues/906",
    addendum = "(accessed: 26.06.2021)",
    keywords = "gluon-ts,DeepAR"
}


@article{m3_vs_m4,
author = {Spiliotis, Evangelos and Kouloumos, Andreas and Assimakopoulos, Vassilis and Makridakis, Spyros},
year = {2018},
month = {12},
pages = {},
title = {Are forecasting competitions data representative of the reality?},
journal = {International Journal of Forecasting},
doi = {10.1016/j.ijforecast.2018.12.007}
}

@article{de_gooijer_25_2006,
	title = {25 years of time series forecasting},
	volume = {22},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207006000021},
	doi = {10.1016/j.ijforecast.2006.01.001},
	abstract = {We review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982–1985 and International Journal of Forecasting 1985–2005). During this period, over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.},
	pages = {443--473},
	number = {3},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {De Gooijer, Jan G. and Hyndman, Rob J.},
	urldate = {2020-06-23},
	date = {2006-01},
	langid = {english},
	file = {25 years of time series forecasting (Gooijer and Hyndman).pdf:/Users/freccero/Documents/sources/25 years of time series forecasting (Gooijer and Hyndman).pdf:application/pdf}
}

@article{ahmed_empirical_2010,
	title = {An Empirical Comparison of Machine Learning Models for Time Series Forecasting},
	volume = {29},
	issn = {0747-4938, 1532-4168},
	url = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2010.481556},
	doi = {10.1080/07474938.2010.481556},
	pages = {594--621},
	number = {5},
	journaltitle = {Econometric Reviews},
	shortjournal = {Econometric Reviews},
	author = {Ahmed, Nesreen K. and Atiya, Amir F. and Gayar, Neamat El and El-Shishiny, Hisham},
	urldate = {2020-06-23},
	date = {2010-08-30},
	langid = {english},
	file = {AN EMPIRICAL COMPARISON OF MACHINE LEARNING MODELS FOR TIME SERIES.pdf:/Users/freccero/Documents/sources/AN EMPIRICAL COMPARISON OF MACHINE LEARNING MODELS FOR TIME SERIES.pdf:application/pdf}
}

@article{feurer_hyperparameter_nodate,
	title = {Hyperparameter Optimization},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning ({AutoML}) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization ({HPO}). In this chapter, we give an overview of the most prominent approaches for {HPO}. We ﬁrst discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-ﬁdelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter setting. Lastly, we point to open problems and future research directions.},
	pages = {35},
	author = {Feurer, Matthias and Hutter, Frank},
	langid = {english},
	file = {chapter1-hpo.pdf:/Users/freccero/Documents/sources/chapter1-hpo.pdf:application/pdf}
}

@article{hyndman_measuring_nodate,
	title = {Measuring forecast accuracy},
	pages = {9},
	author = {Hyndman, Rob J},
	langid = {english},
	file = {ro.pdf:/Users/freccero/Documents/sources/ro.pdf:application/pdf}
}

@article{wang_rule_2009,
	title = {Rule induction for forecasting method selection: Meta-learning the characteristics of univariate time series},
	volume = {72},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231208005134},
	doi = {10.1016/j.neucom.2008.10.017},
	shorttitle = {Rule induction for forecasting method selection},
	abstract = {For univariate forecasting, there are various statistical models and computational algorithms available. In real-world exercises, too many choices can create difficulties in selecting the most appropriate technique, especially for users lacking sufficient knowledge of forecasting. This paper provides evidence, in the form of an empirical study on forecasting accuracy, to show that there is no best single method that can perform well for any given forecasting situation. This study focuses on rule induction for forecasting method selection by understanding the nature of historical forecasting data. A novel approach for selecting a forecasting method for univariate time series based on measurable data characteristics is presented that combines elements of datamining, meta-learning, clustering, classification and statistical measurement. Over 300 datasets are selected for the empirical study from diverse fields. Four popular forecasting methods are used in this study to demonstrate prototype knowledge rules. In order to provide a rich portrait of the global characteristics of the time series, we measure: trend, seasonality, periodicity, serial correlation, skewness, kurtosis, non-linearity, self-similarity, and chaos. The derived rules for selecting the most suitable forecasting method based on these novel characteristic measures can provide references and recommendations for forecasters.},
	pages = {2581--2594},
	number = {10},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Wang, Xiaozhe and Smith-Miles, Kate and Hyndman, Rob},
	urldate = {2020-06-23},
	date = {2009-06},
	langid = {english},
	file = {Rule induction for forecasting method selection- meta-learning the characteristics of univariate time series.pdf:/Users/freccero/Documents/sources/Rule induction for forecasting method selection- meta-learning the characteristics of univariate time series.pdf:application/pdf}
}

@article{makridakis_statistical_2018,
	title = {Statistical and Machine Learning forecasting methods: Concerns and ways forward},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0194889},
	doi = {10.1371/journal.pone.0194889},
	shorttitle = {Statistical and Machine Learning forecasting methods},
	pages = {e0194889},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	editor = {Hernandez Montoya, Alejandro Raul},
	urldate = {2020-06-23},
	date = {2018-03-27},
	langid = {english},
	file = {Statistical and Machine Learning forecasting methods Concerns and ways forward.pdf:/Users/freccero/Documents/sources/Statistical and Machine Learning forecasting methods Concerns and ways forward.pdf:application/pdf}
}

@article{m5,
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilis and Chen, Zhi and Gaba, Anil and Tsetlin, Ilia and Winkler, Robert},
	year = {2020},
	month = {11},
	pages = {},
	title = {The M5 Uncertainty competition: Results, findings and conclusions}
}

@misc{pritzsche_uwe_benchmarking_2015,
	title = {Benchmarking of Classical and Machine-Learning Algorithms (with special emphasis on Bagging and Boosting Approaches) for Time Series Forecasting},
	url = {https://pdfs.semanticscholar.org/719f/e0e9823ee42630f1c663c102074d86354f67.pdf},
	shorttitle = {Benchmarking of Classical and Machine-Learning Algorithms for Time Series Forecasting},
	abstract = {The goal of this Master thesis is to evaluate the time series forecast capability of several Machine Learning approaches, in detail Neural Nets, Random Forests, Kernel Machines (Support Vector Machines and Gaussian Processes), tree-based and component-wise linear and spline-based Boosting, by a comparison with classical {ARIMA} and {ETS} models. For the classical models also time- series specific bagging approaches, Moving Block Bootstrap and Maximum Entropy Bootstrap, are tested. For this purpose, extensive benchmarks are conducted, utilizing the well-known official Tourism, M3 and {NN}5 competition data with the latter comprising also several exogenous covariate effects. In order to uncover specific problems the Machine Learning approaches reveal for the typical time series components of trend and seasonality, a simulation is executed helping in understanding some benchmark results as well as suggesting combinations of the Machine Learning algorithms with classical deseasoning and detrending steps (Box-Cox transformation, {STL} decomposition, seasonal Differencing). Furthermore different multi-step-ahead forecasting strategies are applied to the {NN}5 time series.
It can be shown that {ARIMA} based models are competitive to Machine Learning models for the investigated classical (without any exogenous covariates) time series forecasting situation. On the other hand, {ETS} approaches are less promising. And the classical models can be enhanced by the tested bagging approaches with the easy-to-use Maximum Entropy Bootstrap showing some advantages over the more known Moving Block Bootstrap. One simple but very important result from the conducted simulation using phenotypic time series is represented by the fact that tree-based models as well as splines with a locality property are incapable of modeling a future trend. In such situations these approaches must be combined with a detrending step resulting in inferior results also for the tree-based boosting model (gbm) as one of the most popular Machine Learning algorithm. Actually the Support Vector Machine is the most promising candidate mostly outperforming all other methods including classical approaches, especially in conjunction with exogenous covariates. Even though Gaussian Processes can be founded in the same theoretical context of Kernel machines, this approach is demystified by its results. Further enhancing the Machine Learning models by direct and hybrid (combination of recursive and direct) forecasting strategies does not reveal any substantial improvements for the tested {NN}5 series. Interestingly, the benchmark on the M3 data conducted in this thesis seem to be the first one revealing a more than competitive prediction of special naïve methods for most series making performance conclusions of other studies based on this data highly questionable.},
	author = {Pritzsche, Uwe},
	urldate = {2020-06-23},
	date = {2015-05-27},
	file = {master thesis benchmarking.pdf:/Users/freccero/Documents/sources/master thesis benchmarking.pdf:application/pdf}
}


@article{m3_competition,
	title = {The M3-Competition: results, conclusions and implications},
	journal = {International Journal of Forecasting},
	volume = {16},
	number = {4},
	pages = {451-476},
	year = {2000},
	note = {The M3- Competition},
	issn = {0169-2070},
	doi = {https://doi.org/10.1016/S0169-2070(00)00057-1},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207000000571},
	author = {Spyros Makridakis and Michèle Hibon},
	keywords = {Comparative methods — time series: univariate, Forecasting competitions, M-Competition, Forecasting methods, Forecasting accuracy},
	abstract = {This paper describes the M3-Competition, the latest of the M-Competitions. It explains the reasons for conducting the competition and summarizes its results and conclusions. In addition, the paper compares such results/conclusions with those of the previous two M-Competitions as well as with those of other major empirical studies. Finally, the implications of these results and conclusions are considered, their consequences for both the theory and practice of forecasting are explored and directions for future research are contemplated.}
}

@article{makridakis_m4_2020,
	title = {The M4 Competition: 100,000 time series and 61 forecasting methods},
	volume = {36},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301128},
	doi = {10.1016/j.ijforecast.2019.04.014},
	shorttitle = {The M4 Competition},
	pages = {54--74},
	number = {1},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	urldate = {2020-06-23},
	date = {2020-01},
	langid = {english},
	file = {Full Text:/Users/freccero/Zotero/storage/HPTWNDD3/Makridakis et al. - 2020 - The M4 Competition 100,000 time series and 61 for.pdf:application/pdf}
}

@article{smyl_hybrid_2020,
	title = {A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting},
	volume = {36},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301153},
	doi = {10.1016/j.ijforecast.2019.03.017},
	pages = {75--85},
	number = {1},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Smyl, Slawek},
	urldate = {2020-06-23},
	date = {2020-01},
	langid = {english},
	file = {Full Text:/Users/freccero/Zotero/storage/ZXM2JPP7/Smyl - 2020 - A hybrid method of exponential smoothing and recur.pdf:application/pdf}
}

@article{gluonts_paper,
	title = {{GluonTS}: Probabilistic Time Series Models in Python},
	url = {http://arxiv.org/abs/1906.05264},
	shorttitle = {{GluonTS}},
	abstract = {We introduce Gluon Time Series ({GluonTS}, available at https://gluon-ts.mxnet.io), a library for deep-learning-based time series modeling. {GluonTS} simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy.},
	journaltitle = {{arXiv}:1906.05264 [cs, stat]},
	author = {Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and Türkmen, Ali Caner and Wang, Yuyang},
	urldate = {2020-06-23},
	date = {2019-06-14},
	eprinttype = {arxiv},
	eprint = {1906.05264},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/freccero/Zotero/storage/UPHKA98A/Alexandrov et al. - 2019 - GluonTS Probabilistic Time Series Models in Pytho.pdf:application/pdf;arXiv.org Snapshot:/Users/freccero/Zotero/storage/KYKBQUQ4/1906.html:text/html}
}

@incollection{rangapuram_deep_2018,
	title = {Deep State Space Models for Time Series Forecasting},
	url = {http://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting.pdf},
	pages = {7785--7794},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	date = {2018},
	file = {Rangapuram et al. - 2018 - Deep State Space Models for Time Series Forecastin.pdf:/Users/freccero/Zotero/storage/6RX5WB3H/Rangapuram et al. - 2018 - Deep State Space Models for Time Series Forecastin.pdf:application/pdf}
}

@article{salinas_deepar_2019,
	title = {{DeepAR}: Probabilistic Forecasting with Autoregressive Recurrent Networks},
	url = {http://arxiv.org/abs/1704.04110},
	shorttitle = {{DeepAR}},
	abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose {DeepAR}, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
	journaltitle = {{arXiv}:1704.04110 [cs, stat]},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
	urldate = {2020-06-23},
	date = {2019-02-22},
	eprinttype = {arxiv},
	eprint = {1704.04110},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/freccero/Zotero/storage/HLPZIDHA/Salinas et al. - 2019 - DeepAR Probabilistic Forecasting with Autoregress.pdf:application/pdf;arXiv.org Snapshot:/Users/freccero/Zotero/storage/IID8PZP4/1704.html:text/html}
}

@report{taylor_forecasting_2017,
	title = {Forecasting at scale},
	url = {https://peerj.com/preprints/3190v2},
	abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high quality forecasts — especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
	institution = {{PeerJ} Preprints},
	type = {preprint},
	author = {Taylor, Sean J and Letham, Benjamin},
	urldate = {2020-06-23},
	date = {2017-09-27},
	langid = {english},
	doi = {10.7287/peerj.preprints.3190v2}
}

@inproceedings{huang_benchmarking_2019,
	location = {Los Angeles, {CA}, {USA}},
	title = {Benchmarking Deep Learning for Time Series: Challenges and Directions},
	isbn = {978-1-72810-858-2},
	url = {https://ieeexplore.ieee.org/document/9005496/},
	doi = {10.1109/BigData47090.2019.9005496},
	shorttitle = {Benchmarking Deep Learning for Time Series},
	eventtitle = {2019 {IEEE} International Conference on Big Data (Big Data)},
	pages = {5679--5682},
	booktitle = {2019 {IEEE} International Conference on Big Data (Big Data)},
	publisher = {{IEEE}},
	author = {Huang, Xinyuan and Fox, Geoffrey C. and Serebryakov, Sergey and Mohan, Ankur and Morkisz, Pawel and Dutta, Debojyoti},
	urldate = {2020-06-23},
	date = {2019-12},
	file = {Huang et al. - 2019 - Benchmarking Deep Learning for Time Series Challe.pdf:/Users/freccero/Zotero/storage/HBSB8DB8/Huang et al. - 2019 - Benchmarking Deep Learning for Time Series Challe.pdf:application/pdf}
}

@article{goodwin_asymmetry_1999,
	title = {On the asymmetry of the symmetric {MAPE}},
	volume = {15},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207099000072},
	doi = {10.1016/S0169-2070(99)00007-2},
	pages = {405--408},
	number = {4},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Goodwin, Paul and Lawton, Richard},
	urldate = {2020-06-23},
	date = {1999-10},
	langid = {english},
	file = {Goodwin and Lawton - 1999 - On the asymmetry of the symmetric MAPE.pdf:/Users/freccero/Zotero/storage/HBUS9BHZ/Goodwin and Lawton - 1999 - On the asymmetry of the symmetric MAPE.pdf:application/pdf}
}

@article{hyndman_another_2006,
	title = {Another look at measures of forecast accuracy},
	volume = {22},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207006000239},
	doi = {10.1016/j.ijforecast.2006.03.001},
	pages = {679--688},
	number = {4},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Hyndman, Rob J. and Koehler, Anne B.},
	urldate = {2020-06-23},
	date = {2006-10},
	langid = {english},
	file = {Submitted Version:/Users/freccero/Zotero/storage/SFY7ZJHE/Hyndman and Koehler - 2006 - Another look at measures of forecast accuracy.pdf:application/pdf}
}

@book{hyndman_forecasting_2nd,
	location = {Melbourne, Australia},
	edition = {2nd edition},
	title = {Forecasting: principles and practice},
	url = {https://otexts.com/fpp2/},
	publisher = {{OTexts}},
	author = {Hyndman, Rob J and Athanasopoulos, George},
	urldate = {2020-06-23}
}

@book{hyndman_forecasting_3rd,
	location = {Melbourne, Australia},
	edition = {3nd edition},
	title = {Forecasting: principles and practice},
	url = {https://otexts.com/fpp3/},
	publisher = {{OTexts}},
	author = {Hyndman, Rob J and Athanasopoulos, George},
	urldate = {2021-03-06}
}

@article{srivastava_dropout_2014,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	volume = {15},
	issn = {1532-4435},
	pages = {1929--1958},
	number = {1},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2014-01},
	note = {Publisher: {JMLR}.org},
	keywords = {deep learning, model combination, neural networks, regularization},
	file = {Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:/Users/freccero/Zotero/storage/LJPSAAMF/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@article{IOANNIDIS2020,
title = {Forecasting for COVID-19 has failed},
journal = {International Journal of Forecasting},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2020.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169207020301199},
author = {John P.A. Ioannidis and Sally Cripps and Martin A. Tanner},
keywords = {Forecasting, COVID-19, Mortality, Hospital bed utilization, Bayesian models, SIR models, Bias, Validation},
abstract = {Epidemic forecasting has a dubious track-record, and its failures became more prominent with COVID-19. Poor data input, wrong modeling assumptions, high sensitivity of estimates, lack of incorporation of epidemiological features, poor past evidence on effects of available interventions, lack of transparency, errors, lack of determinacy, consideration of only one or a few dimensions of the problem at hand, lack of expertise in crucial disciplines, groupthink and bandwagon effects, and selective reporting are some of the causes of these failures. Nevertheless, epidemic forecasting is unlikely to be abandoned. Some (but not all) of these problems can be fixed. Careful modeling of predictive distributions rather than focusing on point estimates, considering multiple dimensions of impact, and continuously reappraising models based on their validated performance may help. If extreme values are considered, extremes should be considered for the consequences of multiple dimensions of impact so as to continuously calibrate predictive insights and decision-making. When major decisions (e.g. draconian lockdowns) are based on forecasts, the harms (in terms of health, economy, and society at large) and the asymmetry of risks need to be approached in a holistic fashion, considering the totality of the evidence.}
}

@article{seq2seq,
  author    = {Alex Graves},
  title     = {Generating Sequences With Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1308.0850},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.0850},
  archivePrefix = {arXiv},
  eprint    = {1308.0850},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Graves13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{gluonts-website,
	title = {{GluonTS} - Probabilistic Time Series Modeling},
	url = {https://ts.gluon.ai/},
	titleaddon = {{GluonTS} - Probabilistic Time Series Modeling},
	type = {Documentation},
	urldate = {2021-02-23}
}

@online{gluonts-github,
	title = {{GluonTS} - GitHub repository},
	url = {https://github.com/awslabs/gluon-ts},
	titleaddon = {{GluonTS} - Probabilistic Time Series Modeling},
	type = {Documentation},
	urldate = {2021-02-23}
}

@online{r-forecast-package,
	title = {forecast package in R},
	url = {https://pkg.robjhyndman.com/forecast/},
	type = {Software},
	urldate = {2021-02-23}
}

@online{m4-competitors-guide,
	title = {forecast package in R},
	url = {https://pkg.robjhyndman.com/forecast/},
	type = {Software},
	urldate = {2021-02-23}
}


@article{mattson_mlperf_2020,
	title = {{MLPerf} Training Benchmark},
	url = {http://arxiv.org/abs/1910.01500},
	abstract = {Machine learning ({ML}) needs industry-standard performance benchmarks to support design and competitive evaluation of the many emerging software and hardware solutions for {ML}. But {ML} training presents three unique benchmarking challenges absent from other domains: optimizations that improve training throughput can increase the time to solution, training is stochastic and time to solution exhibits high variance, and software and hardware systems are so diverse that fair benchmarking with the same binary, code, and even hyperparameters is difficult. We therefore present {MLPerf}, an {ML} benchmark that overcomes these challenges. Our analysis quantitatively evaluates {MLPerf}'s efficacy at driving performance and scalability improvements across two rounds of results from multiple vendors.},
	journaltitle = {{arXiv}:1910.01500 [cs, stat]},
	author = {Mattson, Peter and Cheng, Christine and Coleman, Cody and Diamos, Greg and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and Brooks, David and Chen, Dehao and Dutta, Debojyoti and Gupta, Udit and Hazelwood, Kim and Hock, Andrew and Huang, Xinyuan and Ike, Atsushi and Jia, Bill and Kang, Daniel and Kanter, David and Kumar, Naveen and Liao, Jeffery and Ma, Guokai and Narayanan, Deepak and Oguntebi, Tayo and Pekhimenko, Gennady and Pentecost, Lillian and Reddi, Vijay Janapa and Robie, Taylor and John, Tom St and Tabaru, Tsuguchika and Wu, Carole-Jean and Xu, Lingjie and Yamazaki, Masafumi and Young, Cliff and Zaharia, Matei},
	urldate = {2020-07-02},
	date = {2020-03-02},
	eprinttype = {arxiv},
	eprint = {1910.01500},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Performance},
	file = {arXiv Fulltext PDF:/Users/freccero/Zotero/storage/IAFVD8TC/Mattson et al. - 2020 - MLPerf Training Benchmark.pdf:application/pdf;arXiv.org Snapshot:/Users/freccero/Zotero/storage/Q3FZLZXL/1910.html:text/html}
}

@online{vassilieva_deep_nodate,
	title = {Deep Learning Benchmarking Suite},
	url = {https://hewlettpackard.github.io/dlcookbook-dlbs},
	titleaddon = {Deep Learning Benchmarking Suite},
	author = {Vassilieva, Natalia and Serebryakov, Sergey},
	urldate = {2020-07-03}
}

@online{noauthor_mlbench_nodate,
	title = {mlbench: Distributed Machine Learning Benchmark},
	url = {https://mlbench.readthedocs.io/en/latest/},
	abstract = {A public and reproducible collection of reference implementations and benchmark suite for distributed machine learning algorithms, frameworks and systems.},
	titleaddon = {mlbench: Distributed Machine Learning Benchmark},
	urldate = {2020-07-03}
}

@article{cuesta_hyperparameter_2018,
	title = {Hyperparameter Optimization for Large-scale Machine Learning},
	url = {http://rgdoi.net/10.13140/RG.2.2.33876.65927},
	doi = {10.13140/RG.2.2.33876.65927},
	author = {Cuesta, Aitor Palacios},
	urldate = {2020-07-24},
	date = {2018},
	langid = {english},
	note = {Publisher: Unpublished},
	file = {Cuesta - 2018 - Hyperparameter Optimization for Large-scale Machin.pdf:/Users/freccero/Zotero/storage/GWZLLKQZ/Cuesta - 2018 - Hyperparameter Optimization for Large-scale Machin.pdf:application/pdf}
}

@article{derakhshan_continuous_nodate,
	title = {Continuous Deployment of Machine Learning Pipelines},
	abstract = {Today machine learning is entering many business and scienti c applications. The life cycle of machine learning applications consists of data preprocessing for transforming the raw data into features, training a model using the features, and deploying the model for answering prediction queries. In order to guarantee accurate predictions, one has to continuously monitor and update the deployed model and pipeline. Current deployment platforms update the model using online learning methods. When online learning alone is not adequate to guarantee the prediction accuracy, some deployment platforms provide a mechanism for automatic or manual retraining of the model. While the online training is fast, the retraining of the model is time-consuming and adds extra overhead and complexity to the process of deployment. We propose a novel continuous deployment approach for updating the deployed model using a combination of the incoming realtime data and the historical data. We utilize sampling techniques to include the historical data in the training process, thus eliminating the need for retraining the deployed model. We also o er online statistics computation and dynamic materialization of the preprocessed features, which further reduces the total training and data preprocessing time. In our experiments, we design and deploy two pipelines and models to process two real-world datasets. The experiments show that continuous deployment reduces the total training cost up to 15 times while providing the same level of quality when compared to the state-of-the-art deployment approaches.},
	pages = {12},
	author = {Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Rabl, Tilmann and Markl, Volker},
	langid = {english},
	file = {Derakhshan et al. - Continuous Deployment of Machine Learning Pipeline.pdf:/Users/freccero/Zotero/storage/8GLCEG8G/Derakhshan et al. - Continuous Deployment of Machine Learning Pipeline.pdf:application/pdf}
}

@inproceedings{derakhshan_optimizing_2020,
	location = {Portland {OR} {USA}},
	title = {Optimizing Machine Learning Workloads in Collaborative Environments},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389715},
	doi = {10.1145/3318464.3389715},
	abstract = {Effective collaboration among data scientists results in highquality and efficient machine learning ({ML}) workloads. In a collaborative environment, such as Kaggle or Google Colabratory, users typically re-execute or modify published scripts to recreate or improve the result. This introduces many redundant data processing and model training operations. Reusing the data generated by the redundant operations leads to the more efficient execution of future workloads. However, existing collaborative environments lack a data management component for storing and reusing the result of previously executed operations.},
	eventtitle = {{SIGMOD}/{PODS} '20: International Conference on Management of Data},
	pages = {1701--1716},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} International Conference on Management of Data},
	publisher = {{ACM}},
	author = {Derakhshan, Behrouz and Rezaei Mahdiraji, Alireza and Abedjan, Ziawasch and Rabl, Tilmann and Markl, Volker},
	urldate = {2020-07-30},
	date = {2020-06-11},
	langid = {english},
	file = {Derakhshan et al. - 2020 - Optimizing Machine Learning Workloads in Collabora.pdf:/Users/freccero/Zotero/storage/VGEJVCID/Derakhshan et al. - 2020 - Optimizing Machine Learning Workloads in Collabora.pdf:application/pdf}
}

@article{zeuch_analyzing_2019,
	title = {Analyzing efficient stream processing on modern hardware},
	volume = {12},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3303753.3316441},
	doi = {10.14778/3303753.3303758},
	abstract = {Modern Stream Processing Engines ({SPEs}) process large data volumes under tight latency constraints. Many {SPEs} execute processing pipelines using message passing on sharednothing architectures and apply a partition-based scale-out strategy to handle high-velocity input streams. Furthermore, many state-of-the-art {SPEs} rely on a Java Virtual Machine to achieve platform independence and speed up system development by abstracting from the underlying hardware. In this paper, we show that taking the underlying hardware into account is essential to exploit modern hardware eﬃciently. To this end, we conduct an extensive experimental analysis of current {SPEs} and {SPE} design alternatives optimized for modern hardware. Our analysis highlights potential bottlenecks and reveals that state-of-the-art {SPEs} are not capable of fully exploiting current and emerging hardware trends, such as multi-core processors and high-speed networks. Based on our analysis, we describe a set of design changes to the common architecture of {SPEs} to scale-up on modern hardware. We show that the single-node throughput can be increased by up to two orders of magnitude compared to state-of-the-art {SPEs} by applying specialized code generation, fusing operators, batch-style parallelization strategies, and optimized windowing. This speedup allows for deploying typical streaming applications on a single or a few nodes instead of large clusters.},
	pages = {516--530},
	number = {5},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Zeuch, Steffen and Monte, Bonaventura Del and Karimov, Jeyhun and Lutz, Clemens and Renz, Manuel and Traub, Jonas and Breß, Sebastian and Rabl, Tilmann and Markl, Volker},
	urldate = {2020-07-30},
	date = {2019-01-01},
	langid = {english},
	file = {Zeuch et al. - 2019 - Analyzing efficient stream processing on modern ha.pdf:/Users/freccero/Zotero/storage/N3H3AK5X/Zeuch et al. - 2019 - Analyzing efficient stream processing on modern ha.pdf:application/pdf}
}

@inproceedings{del_monte_scalable_2016,
	location = {Nagercoil, India},
	title = {A scalable {GPU}-enabled framework for training deep neural networks},
	isbn = {978-1-4673-6615-1},
	url = {http://ieeexplore.ieee.org/document/7508071/},
	doi = {10.1109/ICGHPC.2016.7508071},
	eventtitle = {2016 2nd International Conference on Green High Performance Computing ({ICGHPC})},
	pages = {1--8},
	booktitle = {2016 2nd International Conference on Green High Performance Computing ({ICGHPC})},
	publisher = {{IEEE}},
	author = {Del Monte, Bonaventura and Prodan, Radu},
	urldate = {2020-07-30},
	date = {2016-02},
	file = {Del Monte and Prodan - 2016 - A scalable GPU-enabled framework for training deep.pdf:/Users/freccero/Zotero/storage/HJIIH26Z/Del Monte and Prodan - 2016 - A scalable GPU-enabled framework for training deep.pdf:application/pdf}
}

@article{willmott_advantages_2005,
	title = {Advantages of the mean absolute error ({MAE}) over the root mean square error ({RMSE}) in assessing average model performance},
	volume = {30},
	issn = {0936-577X, 1616-1572},
	url = {http://www.int-res.com/abstracts/cr/v30/n1/p79-82/},
	doi = {10.3354/cr030079},
	abstract = {The relative abilities of 2, dimensioned statistics — the root-mean-square error ({RMSE}) and the mean absolute error ({MAE}) — to describe average model-performance error are examined. The {RMSE} is of special interest because it is widely reported in the climatic and environmental literature; nevertheless, it is an inappropriate and misinterpreted measure of average error. {RMSE} is inappropriate because it is a function of 3 characteristics of a set of errors, rather than of one (the average error). {RMSE} varies with the variability within the distribution of error magnitudes and with the square root of the number of errors (n1/2), as well as with the average-error magnitude ({MAE}). Our findings indicate that {MAE} is a more natural measure of average error, and (unlike {RMSE}) is unambiguous. Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on {MAE}.},
	pages = {79--82},
	journaltitle = {Climate Research},
	shortjournal = {Clim. Res.},
	author = {Willmott, Cj and Matsuura, K},
	urldate = {2020-07-31},
	date = {2005},
	langid = {english},
	file = {Willmott and Matsuura - 2005 - Advantages of the mean absolute error (MAE) over t.pdf:/Users/freccero/Zotero/storage/5NBE8ZNZ/Willmott and Matsuura - 2005 - Advantages of the mean absolute error (MAE) over t.pdf:application/pdf}
}

@article{makridakis_why_nodate,
	title = {Why Forecasts Fail. What to Do Instead},
	pages = {10},
	author = {Makridakis, Spyros and Hogarth, Robin M and Gaba, Anil},
	langid = {english},
	file = {Accuracy_measures_theoretical_and_practi.pdf:/Users/freccero/Downloads/Accuracy_measures_theoretical_and_practi.pdf:application/pdf}
}

@article{noauthor_conditional_nodate,
	title = {Conditional Image Generation with {PixelCNN} Decoders},
	abstract = {This work explores conditional image generation with a new image density model based on the {PixelCNN} architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the {ImageNet} database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional {PixelCNN} can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of {PixelCNN} to match the state-ofthe-art performance of {PixelRNN} on {ImageNet}, with greatly reduced computational cost.},
	pages = {9},
	langid = {english},
	file = {Conditional Image Generation with.pdf:D\:\\Users\\leona\\Downloads\\Conditional Image Generation with.pdf:application/pdf}
}

@article{yu_high-dimensional_2016,
	title = {High-dimensional Time Series Prediction with Missing Values},
	url = {http://arxiv.org/abs/1509.08333},
	abstract = {High-dimensional time series prediction is needed in applications as diverse as demand forecasting and climatology. Often, such applications require methods that are both highly scalable, and deal with noisy data in terms of corruptions or missing values. Classical time series methods usually fall short of handling both these issues. In this paper, we propose to adapt matrix matrix completion approaches that have previously been successfully applied to large scale noisy data, but which fail to adequately model high-dimensional time series due to temporal dependencies. We present a novel temporal regularized matrix factorization ({TRMF}) framework which supports data-driven temporal dependency learning and enables forecasting ability to our new matrix factorization approach. {TRMF} is highly general, and subsumes many existing matrix factorization approaches for time series data. We make interesting connections to graph regularized matrix factorization methods in the context of learning the dependencies. Experiments on both real and synthetic data show that {TRMF} outperforms several existing approaches for common time series tasks.},
	journaltitle = {{arXiv}:1509.08333 [cs, stat]},
	author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S.},
	urldate = {2020-08-02},
	date = {2016-02-16},
	eprinttype = {arxiv},
	eprint = {1509.08333},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/freccero/Zotero/storage/KEE2PUWA/1509.html:text/html;arXiv Fulltext PDF:/Users/freccero/Zotero/storage/GEVZKTI8/Yu et al. - 2016 - High-dimensional Time Series Prediction with Missi.pdf:application/pdf}
}

@article{bai_empirical_2018,
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as {LSTMs} across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/{TCN} .},
	journaltitle = {{arXiv}:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	urldate = {2020-08-02},
	date = {2018-04-19},
	eprinttype = {arxiv},
	eprint = {1803.01271},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/freccero/Zotero/storage/D24BUMTK/1803.html:text/html;arXiv Fulltext PDF:/Users/freccero/Zotero/storage/WVBMMW5A/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf}
}

@article{ba_layer_2016,
	title = {Layer Normalization},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	journaltitle = {{arXiv}:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	urldate = {2020-08-02},
	date = {2016-07-21},
	eprinttype = {arxiv},
	eprint = {1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/freccero/Zotero/storage/PQXBS5VY/1607.html:text/html;arXiv Fulltext PDF:/Users/freccero/Zotero/storage/USLLH4E6/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf}
}

@article{paine_fast_2016,
	title = {Fast Wavenet Generation Algorithm},
	url = {http://arxiv.org/abs/1611.09482},
	abstract = {This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2{\textasciicircum}L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.},
	journaltitle = {{arXiv}:1611.09482 [cs]},
	author = {Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A. and Huang, Thomas S.},
	urldate = {2020-08-02},
	date = {2016-11-28},
	eprinttype = {arxiv},
	eprint = {1611.09482},
	keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/freccero/Zotero/storage/ZVVW6BQE/1611.html:text/html;arXiv Fulltext PDF:/Users/freccero/Zotero/storage/KFNRBKBV/Paine et al. - 2016 - Fast Wavenet Generation Algorithm.pdf:application/pdf}
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	journaltitle = {{arXiv}:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2020-08-02},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/freccero/Zotero/storage/A6YTU56W/1609.html:text/html;arXiv Fulltext PDF:/Users/freccero/Zotero/storage/L8HPWU27/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf}
}

@article{vaswani_attention_nodate,
	title = {Attention is All you Need},
	url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {11},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	langid = {english},
	file = {Attention Is All You Need.pdf:D\:\\Users\\leona\\Downloads\\Attention Is All You Need.pdf:application/pdf}
}

@article{wen_multi-horizon_2018,
	title = {A Multi-Horizon Quantile Recurrent Forecaster},
	url = {http://arxiv.org/abs/1711.11053},
	abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
	journaltitle = {{arXiv}:1711.11053 [stat]},
	author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
	urldate = {2020-08-02},
	date = {2018-06-28},
	eprinttype = {arxiv},
	eprint = {1711.11053},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/freccero/Zotero/storage/6IJR38AC/1711.html:text/html;arXiv Fulltext PDF:/Users/freccero/Zotero/storage/Q3E8AX4E/Wen et al. - 2018 - A Multi-Horizon Quantile Recurrent Forecaster.pdf:application/pdf}
}

@article{oreshkin_n-beats_2020,
	title = {N-{BEATS}: Neural basis expansion analysis for interpretable time series forecasting},
	url = {http://arxiv.org/abs/1905.10437},
	shorttitle = {N-{BEATS}},
	abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modiﬁcation to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and {TOURISM} competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two conﬁgurations of N-{BEATS} for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year’s winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The ﬁrst conﬁguration of our model does not employ any time-series-speciﬁc components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufﬁcient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
	journaltitle = {{arXiv}:1905.10437 [cs, stat]},
	author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
	urldate = {2020-08-02},
	date = {2020-02-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.10437},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {N-BEATS Neural basis expansion analysis for interpretable time series forecasting.pdf:D\:\\Users\\leona\\Downloads\\N-BEATS Neural basis expansion analysis for interpretable time series forecasting.pdf:application/pdf}
}

@article{lai_modeling_2018,
	title = {Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks},
	url = {http://arxiv.org/abs/1703.07015},
	abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network ({LSTNet}), to address this open challenge. {LSTNet} uses the Convolution Neural Network ({CNN}) and the Recurrent Neural Network ({RNN}) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, {LSTNet} achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
	journaltitle = {{arXiv}:1703.07015 [cs]},
	author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
	urldate = {2020-08-02},
	date = {2018-04-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.07015},
	keywords = {Computer Science - Machine Learning},
	file = {Modeling Long- and Short-Term Temporal Patterns with Deep.pdf:D\:\\Users\\leona\\Downloads\\Modeling Long- and Short-Term Temporal Patterns with Deep.pdf:application/pdf}
}

@article{gortler_visual_2019,
	title = {A Visual Exploration of Gaussian Processes},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
	doi = {10.23915/distill.00017},
	pages = {10.23915/distill.00017},
	number = {4},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
	urldate = {2020-08-02},
	date = {2019-04-02}
}

@article{salinas_high-dimensional_2019,
	title = {High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes},
	url = {http://arxiv.org/abs/1910.03002},
	abstract = {Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, ﬁnancial risk management, causal analysis, or demand forecasting. However, the computational and numerical difﬁculties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an {RNN}-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several realworld datasets that our method provides signiﬁcant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.},
	journaltitle = {{arXiv}:1910.03002 [cs, stat]},
	author = {Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
	urldate = {2020-08-02},
	date = {2019-10-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.03002},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes.pdf:D\:\\Users\\leona\\Downloads\\High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes.pdf:application/pdf}
}

@article{wang_deep_2019,
	title = {Deep Factors for Forecasting},
	url = {http://arxiv.org/abs/1905.12417},
	abstract = {Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically relevant and challenging task. Classical time series models fail to capture complex patterns in the data, and multivariate techniques struggle to scale to large problem sizes. Their reliance on strong structural assumptions makes them data-efﬁcient, and allows them to provide uncertainty estimates. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the beneﬁts of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufﬁcient decomposition of exchangeable time series into a global and a local part. Our experiments demonstrate the advantages of our model both in term of data efﬁciency, accuracy and computational complexity.},
	journaltitle = {{arXiv}:1905.12417 [cs, stat]},
	author = {Wang, Yuyang and Smola, Alex and Maddix, Danielle C. and Gasthaus, Jan and Foster, Dean and Januschowski, Tim},
	urldate = {2020-08-02},
	date = {2019-05-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.12417},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Deep Factors for Forecasting.pdf:D\:\\Users\\leona\\Downloads\\Deep Factors for Forecasting.pdf:application/pdf}
}

@article{green_simple_2015,
	title = {Simple versus complex forecasting: The evidence},
	volume = {68},
	issn = {01482963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014829631500140X},
	doi = {10.1016/j.jbusres.2015.03.026},
	shorttitle = {Simple versus complex forecasting},
	pages = {1678--1685},
	number = {8},
	journaltitle = {Journal of Business Research},
	shortjournal = {Journal of Business Research},
	author = {Green, Kesten C. and Armstrong, J. Scott},
	urldate = {2020-08-25},
	date = {2015-08},
	langid = {english},
	file = {Submitted Version:/Users/freccero/Zotero/storage/W2ARDZXP/Green and Armstrong - 2015 - Simple versus complex forecasting The evidence.pdf:application/pdf}
}

@article{sherstinsky_fundamentals_2020,
	title = {Fundamentals of Recurrent Neural Network ({RNN}) and Long Short-Term Memory ({LSTM}) Network},
	volume = {404},
	issn = {01672789},
	url = {http://arxiv.org/abs/1808.03314},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, {LSTM} networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the {LSTM} network and its parent, {RNN}, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an {RNN} is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential {RNN} and {LSTM} fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical {RNN} formulation from differential equations. We then propose and prove a precise statement, which yields the {RNN} unrolling technique. We also review the difficulties with training the standard {RNN} and address them by transforming the {RNN} into the "Vanilla {LSTM}" network through a series of logical arguments. We provide all equations pertaining to the {LSTM} system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the {LSTM} system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the {LSTM} system and incorporate these extensions into the Vanilla {LSTM} network, producing the most general {LSTM} variant to date. The target reader has already been exposed to {RNNs} and {LSTM} networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented {LSTM} model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
	pages = {132306},
	journaltitle = {Physica D: Nonlinear Phenomena},
	shortjournal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	urldate = {2020-09-24},
	date = {2020-03},
	eprinttype = {arxiv},
	eprint = {1808.03314},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/freccero/Zotero/storage/VBILGV32/Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf:application/pdf;arXiv.org Snapshot:/Users/freccero/Zotero/storage/SRS5YC88/1808.html:text/html}
}


@article{botchkarev_performance_2019,
	title = {Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: Properties and Typology},
	volume = {14},
	issn = {1555-1229, 1555-1237},
	url = {http://arxiv.org/abs/1809.03006},
	doi = {10.28945/4184},
	shorttitle = {Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics},
	abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set.},
	pages = {045--076},
	journaltitle = {Interdisciplinary Journal of Information, Knowledge, and Management},
	shortjournal = {{IJIKM}},
	author = {Botchkarev, Alexei},
	urldate = {2020-11-25},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1809.03006},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/freccero/Zotero/storage/G54LKRDF/Botchkarev - 2019 - Performance Metrics (Error Measures) in Machine Le.pdf:application/pdf;arXiv.org Snapshot:/Users/freccero/Zotero/storage/TVXEWUF8/1809.html:text/html}
}

@article{franses_note_2016,
	title = {A note on the Mean Absolute Scaled Error},
	volume = {32},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207015000448},
	doi = {10.1016/j.ijforecast.2015.03.008},
	abstract = {Hyndman and Koehler (2006) recommend that the Mean Absolute Scaled Error ({MASE}) becomes the standard when comparing forecast accuracy. This note supports their claim by showing that the {MASE} nicely fits within the standard statistical procedures to test equal forecast accuracy initiated in Diebold and Mariano (1995). Various other criteria do not fit as they do not imply the relevant moment properties, and this is illustrated in some simulation experiments.},
	pages = {20--22},
	number = {1},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Franses, Philip Hans},
	urldate = {2020-11-25},
	date = {2016-01},
	langid = {english},
	file = {Franses - 2016 - A note on the Mean Absolute Scaled Error.pdf:/Users/freccero/Zotero/storage/2I2BC7BI/Franses - 2016 - A note on the Mean Absolute Scaled Error.pdf:application/pdf}
}

@article{howard_sequential_2019,
	title = {Sequential estimation of quantiles with applications to A/B-testing and best-arm identification},
	url = {http://arxiv.org/abs/1906.09712},
	abstract = {Consider the problem of sequentially estimating quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We propose new, theoretically sound and practically tight confidence sequences for quantiles, that is, sequences of confidence intervals which are valid uniformly over time. We give two methods for tracking a fixed quantile and two methods for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible \${\textbackslash}sqrt\{t{\textasciicircum}\{-1\} {\textbackslash}log{\textbackslash}log t\}\$ rate, as determined by the law of the iterated logarithm ({LIL}). As a byproduct, we give a non-asymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the {LIL} rate, thus strengthening Smirnov's asymptotic empirical process {LIL}, and extending the famed Dvoretzky-Kiefer-Wolfowitz ({DKW}) inequality to hold uniformly over all sample sizes while only being about twice as wide in practice. This inequality directly yields sequential analogues of the one- and two-sample Kolmogorov-Smirnov tests, and a test of stochastic dominance. We apply our results to the problem of selecting an arm with an approximately best quantile in a multi-armed bandit framework, proving a state-of-the-art sample complexity bound for a novel allocation strategy. Simulations demonstrate that our method stops with fewer samples than existing methods by a factor of five to fifty. Finally, we show how to compute confidence sequences for the difference between quantiles of two arms in an A/B test, along with corresponding always-valid \$p\$-values.},
	journaltitle = {{arXiv}:1906.09712 [math, stat]},
	author = {Howard, Steven R. and Ramdas, Aaditya},
	urldate = {2020-12-06},
	date = {2019-08-23},
	eprinttype = {arxiv},
	eprint = {1906.09712},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Probability, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/freccero/Zotero/storage/QEAMEXGW/Howard and Ramdas - 2019 - Sequential estimation of quantiles with applicatio.pdf:application/pdf;arXiv.org Snapshot:/Users/freccero/Zotero/storage/ETPLTFWI/1906.html:text/html}
}
